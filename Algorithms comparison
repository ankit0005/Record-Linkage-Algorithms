We tested several algorithms to compare strings, and selected the one that would better fit our need.
We compared String A and String B to have metrics on the different algorithms.
Rules for string similarity may differ from case to case. 
If you want to consider “niche” and “chien” similar, you’d use a string similarity algorithm that detects anagrams. Not in our case. 
An app named “niche” and another named “chien” are very likely to be two completely different apps.
After some brainstorming and research we came up with some algorithm methods that would help our case. 
===========================================================================================================================

The Cosine algorithm proved to be irrelevant for us, as for example it does not seem to take into account the letter order, 
which leads to an index of 1 (similar string) on an anagram (“niche” and “chien”).

===========================================================================================================================
Levenshtein Algorithm
The Levenshtein distance is the minimum number of single-character edits required to change one word into the other, so the result is a positive integer, 
sensitive to string length . Which make it more difficult to draw pattern. For example,
the Levenshtein distance between “foo” and “bar” is 3
the Levenshtein distance between “beauties” and “beautiful” is also 3
For us, humans, the “beauties”/”beautiful” pair is much more similar than the “foo”/”bar” pair. But the Levenshtein distance is the same.
The metric we are using here is the inverse of the Levenshtein distance ( 1 / levenshtein_distance ) so the result is a percentage and is more easily read by us. But the problem mentioned above remained the same.
Where 1 is the result of comparing identical strings, “ShazamIphone” and “ShazamAndroid” have a similarity of 0.167. “chien” and “niche” have a similarity of 0.25.
So from the Levenshtein algorithm point of view Chien/Niche are more similar than “ShazamIphone”/“ShazamAndroid” because fewer edits are needed to get from “chien” to “niche”, 
than to get from “ShazamIphone” to “ShazamAndroid”.
===========================================================================================================================
Trigram Comparison
A trigram algorithm is a case of n-gram, a contiguous sequence of n (three, in this case) items from a given sample. 
In our case, an application name is a sample and a character is an item.
So The sequence “martha” has 4 trigrams { mar art rth tha }.
We can use the Trigram method to compare two strings.
Taking for example “martha” and the same word with a typo, “marhta”, and we can compute their trigrams:
Trigrams “martha”: { mar art rth tha }
Trigrams “marhta”: { mar arh rht hta }
To measure similarity we divide the number of matching trigrams in both strings: 1 { mar } by the number of unique trigrams: 7 { mar art rth tha arh rht hta }
The result is 1/7 = 14%
To balance the disadvantage of the outer characters (somewhat to reinforce the similarity of strings starting and ending with the same trigrams), 
we pad the string with blanks on either side resulting in these case in three more trigrams “_ma”, “ha_“ and “ta_”.
Trigrams “ martha ”: { _ma mar art rth tha ha_ }
Trigrams “ marhta ”: { _ma mar arh rht hta ta_ }
Having done that, the number of matching trigrams is up to: 2 { _ma mar }
The number of all unique trigrams: 9 { _ma mar art rth tha arh rht hta ha_ }

The result is now 2/9 = 22%

Using this method to compare “Twitter v1” and “Twitter v2” we have:
The number of matching trigrams: 7 { _tw twi wit itt tte ter er_ }
The number of all unique trigrams: 11 { tw twi wit itt tte ter er_ _v1 _v2 v1_ v2_ }

The result is 7/11 = 63%
The limit of the Trigram method to compare strings is that short strings with one (or two..) different trigrams tend to produce a lower similarity than long ones.
That is how we get a 0.2 similarity between “ShazamAndroid” and “ShazamIphone”, as they have more different trigrams.
The number of matching trigrams is: 5 { _sh sha haz aza zam }
The number of all unique trigrams: 20
As there is a strong dependency with string length, it does not yield a good comparison for us.
===========================================================================================================================
Jaro-Winkler Algorithm

“In computer science and statistics, the Jaro-Winkler distance is a string metric for measuring the edit distance between two sequences.
Informally, the Jaro distance between two words is the minimum number of single-character transpositions required to change one word into the other.
The Jaro-Winkler distance uses a prefix scale which gives more favourable ratings to strings that match from the beginning for a set prefix length”
Source: Wikipedia.
Giving “more importance” to words with identical prefixes made the Jaro-Winkler distance seem very interesting for our use case.
Starting from the beginning with the Jaro distance formula, here how it works. Don’t panic, we go step by step:
The Jaro Distance between two sequences s1 and s2 is defined by:

Jaro distance formula
dj is the Jaro distance
m is the number of matching characters (characters that appear in s1 and in s2)
t is half the number of transpositions (compare the i-th character of s1 and the i-th character of s2 divided by 2)
|s1| is the length of the first string
|s2| is the length of the second string
With a exemple. Let’s take “martha” and “marhta”.
m = 6
t = 2/2 =1 (2 couples of non matching characters, the 4-th and 5-th) { t/h ; h/t }
|s1| = 6
|s2| = 6
Just by replacing numbers is the formula, we get:
dj = (⅓) ( 6/6 + 6/6 + (6–1)/6) = ⅓ 17/6 = 0,944
Jaro distance = 94,4%
Now we know what is the Jaro distance, let’s jump to the Jaro-Winkler distance.
The Jaro-Winkler similarity uses a prefix scale p which gives more favorable ratings to strings that match from the beginning for a set prefix length l.
p is a constant scaling factor for how much the score is adjusted upwards for having common prefixes. The standard value for this constant in Winkler’s work is p=0.1.
l is the length of common prefix at the start of the string (up to a maximum of 4 characters).

Jaro-Winkler distance formula
So back to the “martha”/ “marhta” example, let’s take a prefix lenght of l = 3 (which refers to “mar”). We get to:
dw = 0,944 + ( (0,1*3)(1–0,944)) = 0,944 + 0,3*0,056 = 0,961
Jaro-Winkler distance = 96,1%
Using the JaroWinkler formula we go from the Jaro distance at 94% similarity to 96%.
In our case, most of similar apps start with the same prefix (“twitter v1” vs “twitter v2” or “ShazamIphone” vs “ShazamAndroid” etc. 
See the Algorithms testing table above). So it is an important criterion to take into account.
===========================================================================================================================
The Levenshtein distance:
Also known as edit distance, between two strings is the minimum number of single-character edits to get from one string to the other. 
Allowed edit operations are deletion, insertion, and substitution. Some examples:

number/bumble: 3 (number → bumber → bumblr → bumble)
trying/lying: 2 (trying → rying → lying)
strong/through: 4 (strong → htrong → throng → throug → through)
Some common applications of Levenshtein distance include spelling checkers, computational biology, and speech recognition. 
The default similarity threshold for new SignifAI Decisions is an edit distance of 3 – that can be changed in the Advanced mode of the Decision builder.

===========================================================================================================================
The Jaro-Winkler distance: 
This metric uses a scale of 0-1 to indicate the similarity between two strings, where 0 is no similarity (0 matching characters between strings) and 1 is an exact match. 
Jaro-Winkler similarity takes into account:

Matching: two characters that are the same and in similar positions in the strings. 
Transpositions: matching characters that are in different sequence order in the strings.
Prefix scale: the Jaro-Winkler distance is adjusted favorably if strings match from the beginning (a prefix is up to 4 characters). 

This is a useful metric for cases where identical prefixes are a strong indication of correlation.

===========================================================================================================================
The Longest Common Subsequence distance (LCS)
Is a variation of Levenshtein distance with a more limited set of allowed edit operations.
The LCS distance between two strings is the number of single-character insertions or deletions to change one string into another. 
LCS is most useful in situations where the characters that belong to both strings are the most important – in other words, 
if there’s a lot of “garbage” or noisy characters in your strings, LCS is a useful metric, since it concentrates on the shared characters to determine similarity. 
===========================================================================================================================

The Hamming distance:  
A simpler version of “edit distance” metrics like Levenshtein distance, the Hamming distance between two strings is the number of substitutions required to turn one string into the other. Hamming distance is computed by counting the number of positions with different characters between the two strings. 
For example, in the strings below, the Hamming distance is 2 – notice the underlined “different characters”: flowers/florets

Hamming distance requires the compared strings to be of equal length. 
This is a useful similarity metric for situations where the difference between two strings may be due to typos, or where you want 
to compare two attributes with known lengths (eg. an incident ID or an instance host or Kubernetes pod name in a specific convention). 
===========================================================================================================================
Cosine distance: 
Is another similarity metric we use in our Decision engine to correlate related issues. It’s most commonly used to compare large blocks of text(for example, incident descriptions) and provides an easy visualization of similarity. To obtain the cosine distance between two text blocks, a vector is calculated for each block to represent the count of each unique word in the block. 
The cosine of the two resulting vectors is the cosine distance between them. For example, take these two strings:

            It is not length of life, but depth of life.

              Depth of life does not depend on length.
              
Here are the word counts for these sentences:
it 1 0
is 0 1
not 1 1
length 1 1
of 2 1
life 2 1
but 1 0
depth 1 1
does 0 1
depend 0 1
on 0 1

…and here are those counts represented as a vector:

[1, 0, 1, 1, 2, 2, 1, 1, 0, 0, 0]
[0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1]

The angle between these vectors is about 0.9 (~52 degrees). A cosine distance of 1 is the highest similarity. 
As you can see, this metric is less useful for situations where small character differences in words are insignificant, eg. typos – “nagios” and “nagois” would be treated as completely different words. 
Also, cosine distance ignores word order in the text blocks – for example, these two sentences have a cosine similarity of 1 even though they read completely differently:

this is the first of the sentences, it is unscrambled
the sentences is unscrambled, the first of this it is

====================================================================================================================================================
Fuzzy Score: 

One of my favorites is the Fuzzy Score. Between two strings, a high fuzzy score indicates high similarity. 
The fuzzy score algorithm works by allocating “points” for character matches between strings:

One point for each matching character
Two bonus points for subsequent matches
Note that there are multiple implementation options for Fuzzy score and the above is the most basic one and we have 
implemented multiple more sophisticated Fuzzy algorithms (inspired by SeatGeek) that are working much better on incidents text context.
Example: SignifAI / sgnfai

s: 1
g: 1
n: 1
f: 1
a: 1
i: 1
gn: 2
fa: 2
ai: 2
= 12 points

Fuzzy score is most useful for relatively short strings and when there are non trivial patterns or in a specific order.
